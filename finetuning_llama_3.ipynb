{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Unified Notebook - JUnit Test Generation\n",
    "## Llama-3.1-8B Fine-Tuning for Software Testing\n",
    "\n",
    "**Final Model Results (70K AGGRESSIVE):**\n",
    "- **Precision:** 67.88% âœ…\n",
    "- **Recall:** 75.71% âœ…\n",
    "- **F1-Score:** 69.90% âœ…\n",
    "- **BLEU:** 14.00%\n",
    "\n",
    "**Model Location:** `/workspace/finetuning/final_model_70K_AGGRESSIVE/`\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Contents:\n",
    "1. **Environment Setup** - Package installation and GPU check\n",
    "2. **Data Preparation** - Loading and processing datasets\n",
    "3. **Model Training** - Complete 70K AGGRESSIVE training\n",
    "4. **Evaluation** - Testing on 100 samples\n",
    "5. **Model Merging** - Merge LoRA adapters with base model\n",
    "6. **Inference** - Test the model on new examples\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** This notebook combines all code from the experimental notebook and the successful 70K AGGRESSIVE training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 1: ENVIRONMENT SETUP\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers peft trl datasets bitsandbytes accelerate nltk matplotlib seaborn\n",
    "\n",
    "print(\"âœ… Package installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import Libraries and Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig, \n",
    "    LlamaForCausalLM, \n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, PeftModel, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset, concatenate_datasets, DatasetDict, load_from_disk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure Hugging Face cache location\n",
    "os.environ['HF_HOME'] = '/workspace/finetuning/huggingface'\n",
    "print(f\"âœ“ Models will be cached in: {os.environ['HF_HOME']}\")\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  WARNING: No GPU detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 HuggingFace Login\n",
    "\n",
    "**Required:** You need access to Llama 3.1 model from Meta.\n",
    "1. Request access: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
    "2. Get your token: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Enter your HuggingFace token\n",
    "hf_token = input(\"Enter your HuggingFace token: \")\n",
    "login(token=hf_token)\n",
    "\n",
    "print(\"âœ… Successfully logged in to HuggingFace!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 2: DATA PREPARATION\n",
    "---\n",
    "\n",
    "**Note:** We'll use the already prepared `FYP_dataset_70K_ENHANCED` dataset.\n",
    "\n",
    "If you need to prepare the dataset from scratch, uncomment the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Dataset\n",
    "\n",
    "Loading the enhanced 70K dataset that was used for the best training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the enhanced 70K dataset\n",
    "print(\"Loading FYP_dataset_70K_ENHANCED...\")\n",
    "dataset = load_from_disk(\"FYP_dataset_70K_ENHANCED\")\n",
    "\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"Train samples: {len(dataset['train']):,}\")\n",
    "print(f\"Test samples: {len(dataset['test']):,}\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE TRAINING EXAMPLE:\")\n",
    "print(\"=\"*80)\n",
    "example = dataset['train'][0]\n",
    "for msg in example['messages']:\n",
    "    print(f\"\\n[{msg['role'].upper()}]\")\n",
    "    content = msg['content']\n",
    "    print(content[:300] + \"...\" if len(content) > 300 else content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 3: MODEL TRAINING (70K AGGRESSIVE)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"âœ… Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "print(f\"Vocab size: {len(tokenizer):,}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "eval_data = dataset['test'].select(range(min(500, len(dataset['test']))))\n",
    "\n",
    "print(f\"Train samples: {len(train_data):,}\")\n",
    "print(f\"Eval samples: {len(eval_data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Configure 4-bit Quantization (QLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(\"âœ… QLoRA configuration created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading base model with 4-bit quantization...\")\n",
    "print(\"(This will download ~16GB on first run)\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(f\"âœ… Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n",
    "print(\"âœ… LoRA config created:\")\n",
    "print(f\"   Rank (r): {peft_config.r}\")\n",
    "print(f\"   Alpha: {peft_config.lora_alpha}\")\n",
    "print(f\"   Dropout: {peft_config.lora_dropout}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_70K_AGGRESSIVE',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-4,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.05,\n",
    "    logging_steps=10,\n",
    "    save_steps=250,\n",
    "    eval_steps=250,\n",
    "    eval_strategy='steps',\n",
    "    save_total_limit=3,\n",
    "    bf16=True,\n",
    "    optim='paged_adamw_8bit',\n",
    "    max_grad_norm=0.3,\n",
    "    group_by_length=True,\n",
    "    report_to='none',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "total_steps = (len(train_data) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)) * training_args.num_train_epochs\n",
    "print(f\"\\nTotal training steps: {total_steps:,}\")\n",
    "print(f\"Estimated time: ~6-7 hours\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field='text',\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Start Training\n",
    "\n",
    "**âš ï¸ WARNING:** This will take approximately 6-7 hours!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING 70K AGGRESSIVE TRAINING...\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETED!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./final_model_70K_AGGRESSIVE\"\n",
    "print(f\"\\nSaving model to {output_dir}...\")\n",
    "trainer.save_model(output_dir)\n",
    "\n",
    "print(f\"\\nâœ… Model saved to: {output_dir}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 4: EVALUATION\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Load Trained Model for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"70K AGGRESSIVE MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"\\nLoading tokenizer...\")\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token\n",
    "eval_tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Load base model\n",
    "print(\"Loading base model...\")\n",
    "eval_bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "eval_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    quantization_config=eval_bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "print(\"Loading LoRA adapter from ./final_model_70K_AGGRESSIVE...\")\n",
    "eval_model = PeftModel.from_pretrained(eval_base_model, \"./final_model_70K_AGGRESSIVE\")\n",
    "eval_model.eval()\n",
    "\n",
    "print(\"âœ… Model loaded for evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading test dataset...\")\n",
    "eval_dataset = load_from_disk(\"FYP_dataset_70K_ENHANCED\")\n",
    "test_data = eval_dataset['test']\n",
    "\n",
    "print(f\"Total test samples: {len(test_data):,}\")\n",
    "print(\"Evaluating on 100 samples...\")\n",
    "num_eval_samples = min(100, len(test_data))\n",
    "test_samples = test_data.select(range(num_eval_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_code(code):\n",
    "    \"\"\"Tokenize code into tokens for comparison.\"\"\"\n",
    "    code = re.sub(r'\\s+', ' ', code.strip())\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', code)\n",
    "    return [t.strip() for t in tokens if t.strip()]\n",
    "\n",
    "def calculate_metrics(generated, expected):\n",
    "    \"\"\"Calculate Precision, Recall, F1-Score.\"\"\"\n",
    "    gen_tokens = set(tokenize_code(generated))\n",
    "    exp_tokens = set(tokenize_code(expected))\n",
    "\n",
    "    if not gen_tokens:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    true_positives = len(gen_tokens & exp_tokens)\n",
    "    precision = true_positives / len(gen_tokens) if gen_tokens else 0.0\n",
    "    recall = true_positives / len(exp_tokens) if exp_tokens else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def calculate_bleu(generated, expected):\n",
    "    \"\"\"Calculate BLEU score.\"\"\"\n",
    "    gen_tokens = tokenize_code(generated)\n",
    "    exp_tokens = tokenize_code(expected)\n",
    "\n",
    "    if not gen_tokens or not exp_tokens:\n",
    "        return 0.0\n",
    "\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    return sentence_bleu([exp_tokens], gen_tokens, smoothing_function=smoothing)\n",
    "\n",
    "print(\"âœ… Evaluation metrics defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting evaluation...\\n\")\n",
    "total_precision = 0.0\n",
    "total_recall = 0.0\n",
    "total_f1 = 0.0\n",
    "total_bleu = 0.0\n",
    "\n",
    "for i, example in enumerate(tqdm(test_samples, desc=\"Evaluating\")):\n",
    "    messages = example['messages']\n",
    "    prompt_messages = [msg for msg in messages if msg['role'] != 'assistant']\n",
    "\n",
    "    prompt = eval_tokenizer.apply_chat_template(\n",
    "        prompt_messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = eval_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "    inputs = {k: v.to(eval_model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = eval_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.1,\n",
    "            do_sample=False,\n",
    "            pad_token_id=eval_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_text = eval_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"<|start_header_id|>assistant<|end_header_id|>\" in generated_text:\n",
    "        generated_output = generated_text.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].strip()\n",
    "    else:\n",
    "        prompt_length = len(eval_tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True))\n",
    "        generated_output = generated_text[prompt_length:].strip()\n",
    "\n",
    "    expected_output = messages[-1]['content']\n",
    "\n",
    "    precision, recall, f1 = calculate_metrics(generated_output, expected_output)\n",
    "    bleu = calculate_bleu(generated_output, expected_output)\n",
    "\n",
    "    total_precision += precision\n",
    "    total_recall += recall\n",
    "    total_f1 += f1\n",
    "    total_bleu += bleu\n",
    "\n",
    "    if i == 0:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EXAMPLE OUTPUT (Sample 1):\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"\\n--- EXPECTED ---\")\n",
    "        print(expected_output[:500])\n",
    "        print(\"\\n--- GENERATED ---\")\n",
    "        print(generated_output[:500])\n",
    "        print(\"\\n--- METRICS FOR THIS SAMPLE ---\")\n",
    "        print(f\"Precision: {precision*100:.2f}%\")\n",
    "        print(f\"Recall: {recall*100:.2f}%\")\n",
    "        print(f\"F1-Score: {f1*100:.2f}%\")\n",
    "        print(f\"BLEU: {bleu*100:.2f}%\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"\\nâœ… Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_precision = (total_precision / num_eval_samples) * 100\n",
    "avg_recall = (total_recall / num_eval_samples) * 100\n",
    "avg_f1 = (total_f1 / num_eval_samples) * 100\n",
    "avg_bleu = (total_bleu / num_eval_samples) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS - 70K AGGRESSIVE MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nEvaluated on {num_eval_samples} test samples\")\n",
    "print(f\"\\nAverage Precision: {avg_precision:.2f}%\")\n",
    "print(f\"Average Recall: {avg_recall:.2f}%\")\n",
    "print(f\"Average F1-Score: {avg_f1:.2f}%\")\n",
    "print(f\"Average BLEU Score: {avg_bleu:.2f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 5: MODEL MERGING\n",
    "---\n",
    "\n",
    "**Purpose:** Merge the LoRA adapters with the base model to create a standalone model.\n",
    "\n",
    "**Note:** This section was NOT run in the previous notebook. Run it if you want a merged model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Clear GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear previous models from memory\n",
    "try:\n",
    "    del model\n",
    "    del eval_model\n",
    "    del eval_base_model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"GPU Memory cleared:\")\n",
    "print(f\"  Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "print(f\"  Reserved: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Load Model in FP16 and Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MERGING LORA ADAPTERS WITH BASE MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "base_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "adapter_model = \"./final_model_70K_AGGRESSIVE\"\n",
    "merged_model_path = \"./final_model_70K_AGGRESSIVE_MERGED\"\n",
    "\n",
    "# Load base model in FP16 (without quantization for merging)\n",
    "print(\"\\nLoading base model in FP16...\")\n",
    "merge_model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(\"âœ… Base model loaded\")\n",
    "\n",
    "# Load LoRA adapter\n",
    "print(f\"\\nLoading LoRA adapter from {adapter_model}...\")\n",
    "merge_model = PeftModel.from_pretrained(merge_model, adapter_model)\n",
    "print(\"âœ… LoRA adapter loaded\")\n",
    "\n",
    "# Merge and unload\n",
    "print(\"\\nMerging LoRA weights into base model...\")\n",
    "merge_model = merge_model.merge_and_unload()\n",
    "print(\"âœ… Weights merged!\")\n",
    "\n",
    "# Reload tokenizer\n",
    "print(\"\\nReloading tokenizer...\")\n",
    "merge_tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "merge_tokenizer.pad_token = merge_tokenizer.eos_token\n",
    "merge_tokenizer.padding_side = \"right\"\n",
    "print(\"âœ… Tokenizer loaded\")\n",
    "\n",
    "# Save merged model\n",
    "print(f\"\\nSaving merged model to {merged_model_path}...\")\n",
    "merge_model.save_pretrained(merged_model_path)\n",
    "merge_tokenizer.save_pretrained(merged_model_path)\n",
    "print(\"âœ… Merged model saved!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MERGING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nMerged model location: {merged_model_path}\")\n",
    "print(\"\\nNote: This is a fully merged model (~16GB) that doesn't need LoRA adapters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 6: INFERENCE\n",
    "---\n",
    "\n",
    "**Purpose:** Test the trained model on new examples.\n",
    "\n",
    "**Note:** This section was NOT run in the previous notebook. Run it to test your model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Load Model for Inference\n",
    "\n",
    "**Choose ONE option:**\n",
    "- **Option A:** Load LoRA adapter model (smaller, faster)\n",
    "- **Option B:** Load merged model (if you ran Part 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LOADING MODEL FOR INFERENCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# OPTION A: Load with LoRA adapters (recommended)\n",
    "use_lora = True  # Set to False to use merged model\n",
    "\n",
    "if use_lora:\n",
    "    print(\"\\nLoading model with LoRA adapters...\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    infer_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "    infer_tokenizer.pad_token = infer_tokenizer.eos_token\n",
    "    infer_tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    # Load base model with quantization\n",
    "    infer_bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    infer_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        quantization_config=infer_bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapter\n",
    "    infer_model = PeftModel.from_pretrained(infer_model, \"./final_model_70K_AGGRESSIVE\")\n",
    "    infer_model.eval()\n",
    "    \n",
    "    print(\"âœ… Model loaded with LoRA adapters!\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nLoading merged model...\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    infer_tokenizer = AutoTokenizer.from_pretrained(\"./final_model_70K_AGGRESSIVE_MERGED\")\n",
    "    \n",
    "    # Load merged model\n",
    "    infer_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"./final_model_70K_AGGRESSIVE_MERGED\",\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    infer_model.eval()\n",
    "    \n",
    "    print(\"âœ… Merged model loaded!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL READY FOR INFERENCE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Test on Example Focal Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example focal method to test\n",
    "focal_method = \"\"\"VerificationUtil { \n",
    "    static public boolean isZero(Number value, double zeroThreshold){ \n",
    "        return (value.doubleValue() >= -zeroThreshold) && (value.doubleValue() <= zeroThreshold); \n",
    "    } \n",
    "}\"\"\"\n",
    "\n",
    "print(\"ðŸ”§ Generating test case...\\n\")\n",
    "print(\"=\"*80)\n",
    "print(\"INPUT FOCAL METHOD:\")\n",
    "print(\"=\"*80)\n",
    "print(focal_method)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Generate Test Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare messages in Llama 3.1 format\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an expert Java developer. Generate JUnit test methods for the given focal method. Include @Test annotation and assertions.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Generate a JUnit test method for the following focal method:\\n\\n{focal_method}\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "prompt = infer_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Tokenize\n",
    "inputs = infer_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "inputs = {k: v.to(infer_model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Generate\n",
    "print(\"\\nâ³ Generating (this may take 10-30 seconds)...\\n\")\n",
    "with torch.no_grad():\n",
    "    outputs = infer_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=infer_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode\n",
    "generated_text = infer_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract generated test\n",
    "if \"<|start_header_id|>assistant<|end_header_id|>\" in generated_text:\n",
    "    generated_test = generated_text.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].strip()\n",
    "else:\n",
    "    generated_test = generated_text[len(prompt):].strip()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GENERATED TEST CASE:\")\n",
    "print(\"=\"*80)\n",
    "print(generated_test)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Custom Inference Function\n",
    "\n",
    "Use this function to generate tests for your own focal methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_junit_test(focal_method_code, temperature=0.7, max_tokens=512):\n",
    "    \"\"\"\n",
    "    Generate a JUnit test for the given focal method.\n",
    "    \n",
    "    Args:\n",
    "        focal_method_code (str): The Java focal method code\n",
    "        temperature (float): Sampling temperature (0.1-1.0)\n",
    "        max_tokens (int): Maximum tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated JUnit test method\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an expert Java developer. Generate JUnit test methods for the given focal method. Include @Test annotation and assertions.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Generate a JUnit test method for the following focal method:\\n\\n{focal_method_code}\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    prompt = infer_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = infer_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(infer_model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = infer_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=infer_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = infer_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    if \"<|start_header_id|>assistant<|end_header_id|>\" in generated_text:\n",
    "        generated_test = generated_text.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].strip()\n",
    "    else:\n",
    "        generated_test = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    return generated_test\n",
    "\n",
    "print(\"âœ… generate_junit_test() function defined!\")\n",
    "print(\"\\nUsage:\")\n",
    "print('test = generate_junit_test(your_focal_method)')\n",
    "print('print(test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Try Your Own Focal Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS: Put your own focal method here\n",
    "my_focal_method = \"\"\"\n",
    "public class StringUtils {\n",
    "    public static boolean isEmpty(String str) {\n",
    "        return str == null || str.length() == 0;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Generate test\n",
    "test = generate_junit_test(my_focal_method)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"YOUR FOCAL METHOD:\")\n",
    "print(\"=\"*80)\n",
    "print(my_focal_method)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATED TEST:\")\n",
    "print(\"=\"*80)\n",
    "print(test)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# NOTEBOOK COMPLETE!\n",
    "---\n",
    "\n",
    "## Summary of What This Notebook Does:\n",
    "\n",
    "1. âœ… **Environment Setup** - Installs packages, checks GPU\n",
    "2. âœ… **Data Preparation** - Loads 70K enhanced dataset\n",
    "3. âœ… **Model Training** - Trains 70K AGGRESSIVE model (6-7 hours)\n",
    "4. âœ… **Evaluation** - Tests on 100 samples, calculates metrics\n",
    "5. âœ… **Model Merging** - Merges LoRA adapters with base model\n",
    "6. âœ… **Inference** - Tests model on new focal methods\n",
    "\n",
    "## Key Results:\n",
    "- **Precision:** 67.88%\n",
    "- **Recall:** 75.71%\n",
    "- **F1-Score:** 69.90%\n",
    "- **BLEU:** 14.00%\n",
    "\n",
    "## Model Locations:\n",
    "- LoRA Adapters: `./final_model_70K_AGGRESSIVE/`\n",
    "- Merged Model: `./final_model_70K_AGGRESSIVE_MERGED/` (if Part 5 was run)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for deployment!** ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
